{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from fastai.text import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = requests.get('https://www.gutenberg.org/files/36/36-0.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_iter = re.finditer('BOOK ONE', book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(1067, 1075), match='BOOK ONE'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(start_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting out some print lines here to shrink the notebook -- I did some exploration to figure out where the actual story began and ended (excluding table of contents, acknowledgements, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book[1067:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(1920, 1928), match='BOOK ONE'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(start_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book[1920:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = book[1920:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_iter = re.finditer(\"among the dead.\", book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(64109, 64124), match='among the dead '>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(end_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book[:64125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(345594, 345609), match='among the dead.'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(end_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book[:345609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = book[:345609]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = re.sub(r'[\\r\\n]+', ' ', str(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [i.replace(',', '') for i in book.split('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOOK ONE THE COMING OF THE MARTIANS I',\n",
       " ' THE EVE OF THE WAR',\n",
       " ' No one would have believed in the last years of the nineteenth century that this world was being watched keenly and closely by intelligences greater than manâ\\x80\\x99s and yet as mortal as his own; that as men busied themselves about their various concerns they were scrutinised and studied perhaps almost as narrowly as a man with a microscope might scrutinise the transient creatures that swarm and multiply in a drop of water',\n",
       " ' With infinite complacency men went to and fro over this globe about their little affairs serene in their assurance of their empire over matter',\n",
       " ' It is possible that the infusoria under the microscope do the same',\n",
       " ' No one gave a thought to the older worlds of space as sources of human danger or thought of them only to dismiss the idea of life upon them as impossible or improbable',\n",
       " ' It is curious to recall some of the mental habits of those departed days',\n",
       " ' At most terrestrial men fancied there might be other men upon Mars perhaps inferior to themselves and ready to welcome a missionary enterprise',\n",
       " ' Yet across the gulf of space minds that are to our minds as ours are to those of the beasts that perish intellects vast and cool and unsympathetic regarded this earth with envious eyes and slowly and surely drew their plans against us',\n",
       " ' And early in the twentieth century came the great disillusionment']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! rm book_text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('book_text.csv', 'a+') as file:\n",
    "    for sentence in tokens:\n",
    "        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOK ONE THE COMING OF THE MARTIANS I\n",
      "\n",
      " THE EVE OF THE WAR\n",
      "\n",
      " No one would have believed in the last years of the nineteenth century that this world was being watched keenly and closely by intelligences greater than manâs and yet as mortal as his own; that as men busied themselves about their various concerns they were scrutinised and studied perhaps almost as narrowly as a man with a microscope might scrutinise the transient creatures that swarm and multiply in a drop of water\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('book_text.csv') as file:\n",
    "    lines = file.readlines()[:5]\n",
    "print(lines[0])\n",
    "print(lines[1])\n",
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' And looking across space with instruments and intelligences such as we have scarcely dreamed of they see at its nearest distance only 35000000 of miles sunward of them a morning star of hope our own warmer planet green with vegetation and grey with water with a cloudy atmosphere eloquent of fertility with glimpses through its drifting cloud wisps of broad stretches of populous country and narrow navy-crowded seas'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('book_text.csv').iloc[20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLMDataBunch.from_csv('.', 'book_text.csv', text_cols=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (2434 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj for my own part i had been xxunk excited all day,xxbos xxmaj and through the charred and xxunk xxunk twenty square miles xxunk xxunk the xxmaj martian encampment on xxmaj horsell xxmaj common through charred and ruined villages among the green trees through the blackened and smoking xxunk that had been but a day ago pine xxunk crawled the xxunk scouts with the xxunk that were presently to warn the gunners of the xxmaj martian approach,xxbos xxmaj byfleet was in a tumult ; people packing and a score of hussars some of them dismounted some on xxunk were hunting them about,xxbos âulla ulla ulla ullaâ xxunk that xxunk xxunk waves of sound sweeping down the broad sunlit roadway between the tall buildings on each side,xxbos xxmaj there were half a dozen xxunk or more from the xxmaj woking station standing in the road by the sand - pits a basket - chaise from xxmaj chobham and a rather xxunk carriage\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (609 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj these green xxunk seen none these five or six days but xxmaj iâve no doubt theyâre falling somewhere every night,xxbos xxmaj these noises for the most part xxunk continued intermittently and seemed if anything to increase in number as time wore on,xxbos xxmaj he drank it,xxbos xxmaj they said that they did not know who had xxunk the movements of the troops ; their idea was that a dispute had xxunk at the xxmaj horse xxmaj xxunk,xxbos xxmaj until about midday the xxmaj pool of xxmaj london was an astonishing scene\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj for my own part i had been xxunk excited all day,\n",
       " EmptyLabel )"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   5,  10,  65, ...,   9,   5,  98, 561])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[1][0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    5,   74, 2367,   27,    9,  194,  201,    0, 1310, 1630,   10,   93,  137,  607,   14, 2915,   15,\n",
       "        264,   22,   73, 1872,   25])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.valid_ds[1][0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 70])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  28,    9,    5,  158,  203,  213,   15,    9,   80,   33,    9,  309,\n",
       "           29,  517,   12, 1232,   29,  696,   28,    5,  563,   10,   12,  781,\n",
       "            0, 1233,    2,    5,   48,  214,   13,   68,   50,   42, 1234,   27,\n",
       "            9,  782,   11,    9,  518,   15,  878,    0,   10,  783,   71,  879,\n",
       "            2,    5, 2058,    5,   18,  197, 2059,  134,   62,  191,   12,  564,\n",
       "            2,    5,   36,    0,   45,   10,   31, 1235,  215,   36]),\n",
       " tensor([   9,    5,  158,  203,  213,   15,    9,   80,   33,    9,  309,   29,\n",
       "          517,   12, 1232,   29,  696,   28,    5,  563,   10,   12,  781,    0,\n",
       "         1233,    2,    5,   48,  214,   13,   68,   50,   42, 1234,   27,    9,\n",
       "          782,   11,    9,  518,   15,  878,    0,   10,  783,   71,  879,    2,\n",
       "            5, 2058,    5,   18,  197, 2059,  134,   62,  191,   12,  564,    2,\n",
       "            5,   36,    0,   45,   10,   31, 1235,  215,   36,  405]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3120"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nv = len(data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nh = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3120, 64, 64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nv, nh, bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicLanguageModel(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_h = nn.Linear(nh, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "        self.reset()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"input size: \", x.size())\n",
    "        res = []\n",
    "        h = self.h\n",
    "        for i in range(x.shape[1]):\n",
    "            h = h + self.i_h(x[:,i]) \n",
    "            h = F.relu(self.h_h(h))\n",
    "            res.append(self.bn(h))\n",
    "        print(\"hidden layer size: \", h.size())\n",
    "        print(\"res size: \", res[0].size())\n",
    "        self.h = h.detach()\n",
    "        res = torch.stack(res, dim=1)\n",
    "        print(\"stacked res size: \", res.size())\n",
    "        print(\"output size: \", self.h_o(res).size())\n",
    "        return self.h_o(res)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.h = torch.zeros(nh, nh).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl = BasicLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl.batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelLists;\n",
       "\n",
       "Train: LabelList (2434 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj for my own part i had been xxunk excited all day,xxbos xxmaj and through the charred and xxunk xxunk twenty square miles xxunk xxunk the xxmaj martian encampment on xxmaj horsell xxmaj common through charred and ruined villages among the green trees through the blackened and smoking xxunk that had been but a day ago pine xxunk crawled the xxunk scouts with the xxunk that were presently to warn the gunners of the xxmaj martian approach,xxbos xxmaj byfleet was in a tumult ; people packing and a score of hussars some of them dismounted some on xxunk were hunting them about,xxbos âulla ulla ulla ullaâ xxunk that xxunk xxunk waves of sound sweeping down the broad sunlit roadway between the tall buildings on each side,xxbos xxmaj there were half a dozen xxunk or more from the xxmaj woking station standing in the road by the sand - pits a basket - chaise from xxmaj chobham and a rather xxunk carriage\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (609 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj these green xxunk seen none these five or six days but xxmaj iâve no doubt theyâre falling somewhere every night,xxbos xxmaj these noises for the most part xxunk continued intermittently and seemed if anything to increase in number as time wore on,xxbos xxmaj he drank it,xxbos xxmaj they said that they did not know who had xxunk the movements of the troops ; their idea was that a dispute had xxunk at the xxmaj horse xxmaj xxunk,xxbos xxmaj until about midday the xxmaj pool of xxmaj london was an astonishing scene\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (2434 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj for my own part i had been xxunk excited all day,xxbos xxmaj and through the charred and xxunk xxunk twenty square miles xxunk xxunk the xxmaj martian encampment on xxmaj horsell xxmaj common through charred and ruined villages among the green trees through the blackened and smoking xxunk that had been but a day ago pine xxunk crawled the xxunk scouts with the xxunk that were presently to warn the gunners of the xxmaj martian approach,xxbos xxmaj byfleet was in a tumult ; people packing and a score of hussars some of them dismounted some on xxunk were hunting them about,xxbos âulla ulla ulla ullaâ xxunk that xxunk xxunk waves of sound sweeping down the broad sunlit roadway between the tall buildings on each side,xxbos xxmaj there were half a dozen xxunk or more from the xxmaj woking station standing in the road by the sand - pits a basket - chaise from xxmaj chobham and a rather xxunk carriage\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (609 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj these green xxunk seen none these five or six days but xxmaj iâve no doubt theyâre falling somewhere every night,xxbos xxmaj these noises for the most part xxunk continued intermittently and seemed if anything to increase in number as time wore on,xxbos xxmaj he drank it,xxbos xxmaj they said that they did not know who had xxunk the movements of the troops ; their idea was that a dispute had xxunk at the xxmaj horse xxmaj xxunk,xxbos xxmaj until about midday the xxmaj pool of xxmaj london was an astonishing scene\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.TextLMDataBunch"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-48a0a2aae35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'arch' is not defined"
     ]
    }
   ],
   "source": [
    "learn = LanguageLearner(data, model=, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  torch.Size([1, 70])\n",
      "hidden layer size:  torch.Size([64, 64])\n",
      "res size:  torch.Size([64, 64])\n",
      "stacked res size:  torch.Size([64, 70, 64])\n",
      "output size:  torch.Size([64, 70, 3120])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BasicLanguageModel\n",
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Embedding            [64]                 199,680    True      \n",
       "______________________________________________________________________\n",
       "Linear               [64]                 4,160      True      \n",
       "______________________________________________________________________\n",
       "Linear               [70, 3120]           202,800    True      \n",
       "______________________________________________________________________\n",
       "BatchNorm1d          [64]                 128        True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 406,768\n",
       "Total trainable params: 406,768\n",
       "Total non-trainable params: 0\n",
       "Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99)\n",
       "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
       "Loss function : FlattenedLoss\n",
       "======================================================================\n",
       "Callbacks functions applied \n",
       "    RNNTrainer"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sailors could no longer come up the xxmaj thames they came on to the xxmaj essex coast to xxmaj harwich and xxmaj walton and xxmaj xxunk and afterwards to xxmaj foulness and xxmaj xxunk to bring off the people xxbos â i resolved to leave xxunk that i had ! xxmaj xxunk now for the xxunk xxunk i xxunk out food and drink xxbos xxmaj haggard special xxunk with white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>way!â xxmaj it was like riding into the smoke of a fire to approach the meeting point of the lane and road ; the crowd xxunk like a fire and the dust was hot and pungent xxbos âulla ulla ulla ullaâ cried the voice coming as it seemed to me from the district about xxmaj regentâs xxmaj park xxbos xxmaj its physical condition is still xxunk a xxunk but we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>two days of xxunk and rain and at xxmaj clapham xxmaj junction the line had been wrecked again ; there were hundreds of out - of - work clerks and shopmen working side by side with the xxunk navvies and we were jolted over a hasty xxunk xxbos â âwhat xxunk xxunk xxunk âno xxunk and the artilleryman began a vivid account of the xxmaj heat - xxmaj ray xxbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>curate and stopped at the scullery door xxbos i whispered for the curate several times and at last felt my way to the door of the kitchen xxbos xxmaj we got to xxmaj hampton xxmaj court without misadventure our minds full of strange and unfamiliar appearances and at xxmaj hampton xxmaj court our eyes were relieved to find a patch of green that had escaped the xxunk xxunk xxbos i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>road xxbos xxmaj iâve been in sight of death once or twice ; xxmaj iâm not an xxunk soldier and at the best and worst xxunk just death xxbos xxmaj that xxunk up xxbos xxmaj the vapour did not xxunk as a true gas would do xxbos xxmaj close on the rear of this came a couple of cabs the xxunk of a long xxunk of flying vehicles going for</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.data.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:  torch.Size([64, 70])\n",
      "hidden layer size:  torch.Size([64, 64])\n",
      "res size:  torch.Size([64, 64])\n",
      "stacked res size:  torch.Size([64, 70, 64])\n",
      "output size:  torch.Size([64, 70, 3120])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (70) to match target batch_size (4480).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-0f80aab506fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mto_detach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input, target, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatify\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_2d\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCrossEntropyFlat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1788\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (70) to match target batch_size (4480)."
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type BasicLanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "learn.export(file=\"export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mdl = load_learner(\".\", \"export.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-0d01006abf8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, item, return_x, batch_first, with_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrab_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mEmptyLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCategoryProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreProcessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/data.py\u001b[0m in \u001b[0;36mreconstruct\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0midx_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0midx_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx_max\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx_max\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/transform.py\u001b[0m in \u001b[0;36mtextify\u001b[0;34m(self, nums, sep)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtextify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;34m\"Convert a list of `nums` to their tokens.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/transform.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtextify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;34m\"Convert a list of `nums` to their tokens.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "pred_mdl.predict(\"The \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:18 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>6.879877</th>\n",
       "    <th>6.727090</th>\n",
       "    <th>0.085993</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>6.634016</th>\n",
       "    <th>5.958604</th>\n",
       "    <th>0.175949</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>6.021026</th>\n",
       "    <th>4.314600</th>\n",
       "    <th>0.229241</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>5.372983</th>\n",
       "    <th>3.986629</th>\n",
       "    <th>0.256306</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>4.911340</th>\n",
       "    <th>3.886966</th>\n",
       "    <th>0.264955</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>4.575227</th>\n",
       "    <th>3.848140</th>\n",
       "    <th>0.276897</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>4.319477</th>\n",
       "    <th>3.857784</th>\n",
       "    <th>0.276116</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>4.118579</th>\n",
       "    <th>3.862509</th>\n",
       "    <th>0.277790</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.954926</th>\n",
       "    <th>3.927150</th>\n",
       "    <th>0.266853</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.819950</th>\n",
       "    <th>3.912256</th>\n",
       "    <th>0.280134</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.704048</th>\n",
       "    <th>3.952745</th>\n",
       "    <th>0.280246</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.602880</th>\n",
       "    <th>3.948744</th>\n",
       "    <th>0.282366</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.510496</th>\n",
       "    <th>3.988242</th>\n",
       "    <th>0.278460</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.426747</th>\n",
       "    <th>4.030641</th>\n",
       "    <th>0.274386</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.351410</th>\n",
       "    <th>4.073006</th>\n",
       "    <th>0.273996</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.283318</th>\n",
       "    <th>4.082377</th>\n",
       "    <th>0.278348</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.223031</th>\n",
       "    <th>4.083824</th>\n",
       "    <th>0.273549</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.172545</th>\n",
       "    <th>4.091926</th>\n",
       "    <th>0.275000</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.129105</th>\n",
       "    <th>4.088652</th>\n",
       "    <th>0.279129</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.094897</th>\n",
       "    <th>4.093309</th>\n",
       "    <th>0.277958</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_mdl = LanguageLearner(data, BasicLanguageModel(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type BasicLanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (2434 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj to get under water ! xxmaj that was it ! xxunk under xxunk i shouted xxunk,xxbos i turned and running xxunk made for the first group of trees perhaps a hundred yards away ; but i ran xxunk and xxunk for i could not xxunk my face from these things,xxbos xxmaj it was with the greatest xxunk they could get her down to the xxunk where presently my brother succeeded in xxunk the attention of some men on a xxunk steamer from the xxmaj thames,xxbos xxmaj he looked back saw what i was doing and turned to xxunk me,xxbos xxmaj among these were a couple of xxunk a xxunk xxunk i xxunk xxunk a xxunk carrying a xxunk xxmaj xxunk the xxunk and his little boy and two or three xxunk and xxunk xxunk who were xxunk to xxunk about the railway station\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (609 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj now no xxunk human being saw so much of the xxmaj martians in action as i did,xxbos xxmaj in the xxunk of bright xxunk sunlight i saw the xxmaj martian in its xxmaj xxunk of a handling - machine xxunk the xxunk head,xxbos xxmaj the greater part of the xxunk was the brain xxunk enormous xxunk to the eyes xxunk and xxunk tentacles,xxbos xxmaj the xxmaj black xxmaj smoke xxunk slowly xxunk all through xxmaj xxunk morning xxunk nearer and nearer to us driving at last along the xxunk outside the house that hid us,xxbos xxmaj going on along the xxmaj xxunk to xxmaj xxunk xxmaj xxunk the paper in his hand my brother saw some of the fugitives from xxmaj west xxmaj surrey\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BasicLanguageModel(\n",
       "  (i_h): Embedding(1006, 64)\n",
       "  (h_h): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (h_o): Linear(in_features=64, out_features=1006, bias=True)\n",
       "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fe84d256510>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True)], callbacks=[RNNTrainer\n",
       "learn: LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (2434 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj to get under water ! xxmaj that was it ! xxunk under xxunk i shouted xxunk,xxbos i turned and running xxunk made for the first group of trees perhaps a hundred yards away ; but i ran xxunk and xxunk for i could not xxunk my face from these things,xxbos xxmaj it was with the greatest xxunk they could get her down to the xxunk where presently my brother succeeded in xxunk the attention of some men on a xxunk steamer from the xxmaj thames,xxbos xxmaj he looked back saw what i was doing and turned to xxunk me,xxbos xxmaj among these were a couple of xxunk a xxunk xxunk i xxunk xxunk a xxunk carrying a xxunk xxmaj xxunk the xxunk and his little boy and two or three xxunk and xxunk xxunk who were xxunk to xxunk about the railway station\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (609 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj now no xxunk human being saw so much of the xxmaj martians in action as i did,xxbos xxmaj in the xxunk of bright xxunk sunlight i saw the xxmaj martian in its xxmaj xxunk of a handling - machine xxunk the xxunk head,xxbos xxmaj the greater part of the xxunk was the brain xxunk enormous xxunk to the eyes xxunk and xxunk tentacles,xxbos xxmaj the xxmaj black xxmaj smoke xxunk slowly xxunk all through xxmaj xxunk morning xxunk nearer and nearer to us driving at last along the xxunk outside the house that hid us,xxbos xxmaj going on along the xxmaj xxunk to xxmaj xxunk xxmaj xxunk the paper in his hand my brother saw some of the fugitives from xxmaj west xxmaj surrey\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BasicLanguageModel(\n",
       "  (i_h): Embedding(1006, 64)\n",
       "  (h_h): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (h_o): Linear(in_features=64, out_features=1006, bias=True)\n",
       "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fe84d256510>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(1006, 64)\n",
       "  (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (2): Linear(in_features=64, out_features=1006, bias=True)\n",
       "  (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(1006, 64)\n",
       "  (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (2): Linear(in_features=64, out_features=1006, bias=True)\n",
       "  (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")], add_time=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mdl.load(\"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7953c296c37a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The martian \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text, n_words, no_unk, temperature, min_p, sep, decoder)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m#if len(new_idx) == 0: self.model[0].select_hidden([0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mno_unk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUNK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_p\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "pred_mdl.predict(\"The martian \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCustom(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = nn.BatchNorm1d(nh)\n",
    "        self.h = torch.zeros(bs, nh).cuda()\n",
    "        self.cell_state = torch.zeros(bs, nh).cuda()\n",
    "        self.forget_gate = nn.Linear(nh*2, nh)\n",
    "        self.input_gate = nn.Linear(nh*2, nh)\n",
    "        self.candidate_gate = nn.Linear(nh*2, nh)\n",
    "        self.output_gate = nn.Linear(nh*2, nh)\n",
    "        self.combine_gate = nn.Linear(nh*2, nh)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        h = self.h\n",
    "        cell_state = self.cell_state\n",
    "        for i in range(x.shape[1]):\n",
    "            x_i = self.i_h(x[:,i])\n",
    "            concat = torch.cat([x_i, h], 1)\n",
    "            forget_gate = nn.Sigmoid()(self.forget_gate(concat))\n",
    "            cell_state = forget_gate * cell_state\n",
    "            input_gate = nn.Sigmoid()(self.input_gate(concat))\n",
    "            candidate_gate = nn.Tanh()(self.candidate_gate(concat))\n",
    "            cell_state = cell_state + (input_gate * candidate_gate)\n",
    "            output_gate = nn.Sigmoid()(self.output_gate(concat))\n",
    "            combine_gate = nn.Tanh()(cell_state)\n",
    "            h = combine_gate * output_gate \n",
    "            res.append(h)\n",
    "        self.h = h.detach()\n",
    "        self.cell_state = cell_state.detach()\n",
    "        res = torch.stack(res, dim=1)\n",
    "        res = self.h_o(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn = Learner(data, LSTMCustom(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:26 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>6.864742</th>\n",
       "    <th>6.792367</th>\n",
       "    <th>0.159208</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>6.635358</th>\n",
       "    <th>5.472675</th>\n",
       "    <th>0.191797</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>5.937733</th>\n",
       "    <th>4.743526</th>\n",
       "    <th>0.221205</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>5.513904</th>\n",
       "    <th>4.531507</th>\n",
       "    <th>0.221205</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>5.205496</th>\n",
       "    <th>4.341786</th>\n",
       "    <th>0.235435</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>4.955205</th>\n",
       "    <th>4.181851</th>\n",
       "    <th>0.248047</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>4.741245</th>\n",
       "    <th>4.059894</th>\n",
       "    <th>0.255078</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>4.562502</th>\n",
       "    <th>3.984762</th>\n",
       "    <th>0.255246</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>4.410934</th>\n",
       "    <th>3.929970</th>\n",
       "    <th>0.263114</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>4.282375</th>\n",
       "    <th>3.887057</th>\n",
       "    <th>0.265848</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>4.172085</th>\n",
       "    <th>3.856799</th>\n",
       "    <th>0.267913</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>4.079171</th>\n",
       "    <th>3.840784</th>\n",
       "    <th>0.276507</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>4.000644</th>\n",
       "    <th>3.824120</th>\n",
       "    <th>0.280022</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.933728</th>\n",
       "    <th>3.813533</th>\n",
       "    <th>0.282478</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.876935</th>\n",
       "    <th>3.804918</th>\n",
       "    <th>0.283984</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.831275</th>\n",
       "    <th>3.797330</th>\n",
       "    <th>0.284040</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.793629</th>\n",
       "    <th>3.794198</th>\n",
       "    <th>0.285658</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.763190</th>\n",
       "    <th>3.793282</th>\n",
       "    <th>0.285993</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.737945</th>\n",
       "    <th>3.792296</th>\n",
       "    <th>0.285938</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.718555</th>\n",
       "    <th>3.792588</th>\n",
       "    <th>0.285770</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:28 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.656716</th>\n",
       "    <th>3.791082</th>\n",
       "    <th>0.285658</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.654055</th>\n",
       "    <th>3.788638</th>\n",
       "    <th>0.286272</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.651327</th>\n",
       "    <th>3.783318</th>\n",
       "    <th>0.285435</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.645024</th>\n",
       "    <th>3.777210</th>\n",
       "    <th>0.286272</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.636613</th>\n",
       "    <th>3.772876</th>\n",
       "    <th>0.284040</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.623350</th>\n",
       "    <th>3.766766</th>\n",
       "    <th>0.286440</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.605362</th>\n",
       "    <th>3.756979</th>\n",
       "    <th>0.289732</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.584810</th>\n",
       "    <th>3.751713</th>\n",
       "    <th>0.292969</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.562498</th>\n",
       "    <th>3.750383</th>\n",
       "    <th>0.287891</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.539610</th>\n",
       "    <th>3.745222</th>\n",
       "    <th>0.290569</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.515432</th>\n",
       "    <th>3.743086</th>\n",
       "    <th>0.292411</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.490983</th>\n",
       "    <th>3.739726</th>\n",
       "    <th>0.292578</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.468776</th>\n",
       "    <th>3.737951</th>\n",
       "    <th>0.293415</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.447287</th>\n",
       "    <th>3.738633</th>\n",
       "    <th>0.294196</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.428099</th>\n",
       "    <th>3.738045</th>\n",
       "    <th>0.293973</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.410581</th>\n",
       "    <th>3.736229</th>\n",
       "    <th>0.294922</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.395247</th>\n",
       "    <th>3.735841</th>\n",
       "    <th>0.294252</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.381508</th>\n",
       "    <th>3.736081</th>\n",
       "    <th>0.294699</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.371768</th>\n",
       "    <th>3.736112</th>\n",
       "    <th>0.295257</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.363322</th>\n",
       "    <th>3.735982</th>\n",
       "    <th>0.294643</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:27 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.336965</th>\n",
       "    <th>3.736379</th>\n",
       "    <th>0.294754</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.337911</th>\n",
       "    <th>3.735885</th>\n",
       "    <th>0.293973</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.335223</th>\n",
       "    <th>3.737562</th>\n",
       "    <th>0.295480</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.334676</th>\n",
       "    <th>3.745260</th>\n",
       "    <th>0.295926</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.333137</th>\n",
       "    <th>3.748382</th>\n",
       "    <th>0.296094</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.328768</th>\n",
       "    <th>3.753780</th>\n",
       "    <th>0.291908</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.321503</th>\n",
       "    <th>3.758725</th>\n",
       "    <th>0.294587</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.309969</th>\n",
       "    <th>3.761811</th>\n",
       "    <th>0.293806</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.294904</th>\n",
       "    <th>3.767322</th>\n",
       "    <th>0.292578</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.279323</th>\n",
       "    <th>3.770055</th>\n",
       "    <th>0.294141</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.262486</th>\n",
       "    <th>3.775835</th>\n",
       "    <th>0.291350</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.245181</th>\n",
       "    <th>3.776443</th>\n",
       "    <th>0.292857</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.228091</th>\n",
       "    <th>3.776336</th>\n",
       "    <th>0.293750</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.210935</th>\n",
       "    <th>3.777239</th>\n",
       "    <th>0.293750</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.194838</th>\n",
       "    <th>3.779726</th>\n",
       "    <th>0.294978</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.179739</th>\n",
       "    <th>3.778537</th>\n",
       "    <th>0.292690</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.166393</th>\n",
       "    <th>3.780622</th>\n",
       "    <th>0.295312</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.155432</th>\n",
       "    <th>3.778791</th>\n",
       "    <th>0.294308</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.147031</th>\n",
       "    <th>3.779322</th>\n",
       "    <th>0.294085</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.140848</th>\n",
       "    <th>3.779990</th>\n",
       "    <th>0.294196</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:27 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.114398</th>\n",
       "    <th>3.778969</th>\n",
       "    <th>0.294141</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.115606</th>\n",
       "    <th>3.779258</th>\n",
       "    <th>0.294364</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.115293</th>\n",
       "    <th>3.779570</th>\n",
       "    <th>0.294085</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.114501</th>\n",
       "    <th>3.780993</th>\n",
       "    <th>0.293304</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.115257</th>\n",
       "    <th>3.780989</th>\n",
       "    <th>0.293973</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.113943</th>\n",
       "    <th>3.781930</th>\n",
       "    <th>0.294587</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.113512</th>\n",
       "    <th>3.782819</th>\n",
       "    <th>0.294085</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.113523</th>\n",
       "    <th>3.784273</th>\n",
       "    <th>0.293694</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.111033</th>\n",
       "    <th>3.784204</th>\n",
       "    <th>0.292690</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.110197</th>\n",
       "    <th>3.786014</th>\n",
       "    <th>0.294141</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.107783</th>\n",
       "    <th>3.785646</th>\n",
       "    <th>0.293304</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.105576</th>\n",
       "    <th>3.787341</th>\n",
       "    <th>0.293583</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.104324</th>\n",
       "    <th>3.786936</th>\n",
       "    <th>0.293359</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.102350</th>\n",
       "    <th>3.787644</th>\n",
       "    <th>0.293304</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.100884</th>\n",
       "    <th>3.787629</th>\n",
       "    <th>0.293806</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.099768</th>\n",
       "    <th>3.787719</th>\n",
       "    <th>0.293694</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.098864</th>\n",
       "    <th>3.788159</th>\n",
       "    <th>0.293806</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.098070</th>\n",
       "    <th>3.787970</th>\n",
       "    <th>0.293192</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.097750</th>\n",
       "    <th>3.788239</th>\n",
       "    <th>0.294085</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.097582</th>\n",
       "    <th>3.787933</th>\n",
       "    <th>0.293583</th>\n",
       "    <th>00:01</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 1006 64\n"
     ]
    }
   ],
   "source": [
    "print(bs, nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i_h = nn.Embedding(nv, nh)\n",
    "        self.h_o = nn.Linear(nh, nv)\n",
    "        self.bn = BatchNorm1dFlat(nh)\n",
    "        self.h = torch.zeros(1, bs, nh).cuda()\n",
    "        self.c = torch.zeros(1, bs, nh).cuda()\n",
    "        self.lstm = nn.LSTM(nh, nh, 1, batch_first=True, dropout=0.5) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        res, (h, c) = self.lstm(self.i_h(x), (self.h, self.c))\n",
    "        self.h, self.c = h.detach(), c.detach()\n",
    "        return self.h_o(self.bn(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(data, LSTMModel(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:04 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>6.951297</th>\n",
       "    <th>6.906593</th>\n",
       "    <th>0.004632</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>6.911371</th>\n",
       "    <th>6.844083</th>\n",
       "    <th>0.048884</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>6.827812</th>\n",
       "    <th>6.621582</th>\n",
       "    <th>0.110993</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>6.675880</th>\n",
       "    <th>6.158518</th>\n",
       "    <th>0.167969</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>6.434752</th>\n",
       "    <th>5.493100</th>\n",
       "    <th>0.195033</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>6.070967</th>\n",
       "    <th>4.755964</th>\n",
       "    <th>0.215123</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>5.640584</th>\n",
       "    <th>4.252319</th>\n",
       "    <th>0.245592</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>5.245998</th>\n",
       "    <th>4.046191</th>\n",
       "    <th>0.266071</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>4.915372</th>\n",
       "    <th>3.936378</th>\n",
       "    <th>0.271540</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>4.647189</th>\n",
       "    <th>3.870053</th>\n",
       "    <th>0.276507</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>4.428436</th>\n",
       "    <th>3.836051</th>\n",
       "    <th>0.279967</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>4.251418</th>\n",
       "    <th>3.819799</th>\n",
       "    <th>0.281027</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>4.107474</th>\n",
       "    <th>3.799559</th>\n",
       "    <th>0.283538</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.990337</th>\n",
       "    <th>3.789144</th>\n",
       "    <th>0.285714</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.894977</th>\n",
       "    <th>3.783965</th>\n",
       "    <th>0.284933</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.817564</th>\n",
       "    <th>3.779047</th>\n",
       "    <th>0.285658</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.756182</th>\n",
       "    <th>3.773753</th>\n",
       "    <th>0.287388</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.704999</th>\n",
       "    <th>3.773361</th>\n",
       "    <th>0.286551</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.666897</th>\n",
       "    <th>3.772787</th>\n",
       "    <th>0.286830</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.636200</th>\n",
       "    <th>3.772675</th>\n",
       "    <th>0.286440</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:04 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.530246</th>\n",
       "    <th>3.771509</th>\n",
       "    <th>0.287277</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.527559</th>\n",
       "    <th>3.770522</th>\n",
       "    <th>0.286328</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.522986</th>\n",
       "    <th>3.766570</th>\n",
       "    <th>0.286607</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.513874</th>\n",
       "    <th>3.760659</th>\n",
       "    <th>0.288504</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.498860</th>\n",
       "    <th>3.758373</th>\n",
       "    <th>0.286607</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.479750</th>\n",
       "    <th>3.761188</th>\n",
       "    <th>0.287891</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.454956</th>\n",
       "    <th>3.765979</th>\n",
       "    <th>0.284933</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.424772</th>\n",
       "    <th>3.756758</th>\n",
       "    <th>0.289788</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.393285</th>\n",
       "    <th>3.761786</th>\n",
       "    <th>0.289788</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.358889</th>\n",
       "    <th>3.772546</th>\n",
       "    <th>0.286886</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.325923</th>\n",
       "    <th>3.775745</th>\n",
       "    <th>0.287165</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.292899</th>\n",
       "    <th>3.790769</th>\n",
       "    <th>0.286607</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.261122</th>\n",
       "    <th>3.787488</th>\n",
       "    <th>0.292187</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.230368</th>\n",
       "    <th>3.793335</th>\n",
       "    <th>0.288783</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.202432</th>\n",
       "    <th>3.793818</th>\n",
       "    <th>0.291350</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.177390</th>\n",
       "    <th>3.796495</th>\n",
       "    <th>0.289900</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.156401</th>\n",
       "    <th>3.800003</th>\n",
       "    <th>0.289397</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.138763</th>\n",
       "    <th>3.800703</th>\n",
       "    <th>0.290458</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.124307</th>\n",
       "    <th>3.799155</th>\n",
       "    <th>0.291071</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.113371</th>\n",
       "    <th>3.800400</th>\n",
       "    <th>0.290960</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:04 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.077008</th>\n",
       "    <th>3.799859</th>\n",
       "    <th>0.290904</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.076132</th>\n",
       "    <th>3.803214</th>\n",
       "    <th>0.289788</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.075212</th>\n",
       "    <th>3.801907</th>\n",
       "    <th>0.290290</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.074474</th>\n",
       "    <th>3.802700</th>\n",
       "    <th>0.290904</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.073398</th>\n",
       "    <th>3.804970</th>\n",
       "    <th>0.291629</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.072807</th>\n",
       "    <th>3.806811</th>\n",
       "    <th>0.291518</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.070864</th>\n",
       "    <th>3.807528</th>\n",
       "    <th>0.290737</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.069525</th>\n",
       "    <th>3.806668</th>\n",
       "    <th>0.290569</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.067733</th>\n",
       "    <th>3.811751</th>\n",
       "    <th>0.289621</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.064878</th>\n",
       "    <th>3.811629</th>\n",
       "    <th>0.290290</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.061129</th>\n",
       "    <th>3.813105</th>\n",
       "    <th>0.290011</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.057139</th>\n",
       "    <th>3.814528</th>\n",
       "    <th>0.289397</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.053804</th>\n",
       "    <th>3.816406</th>\n",
       "    <th>0.290848</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.052228</th>\n",
       "    <th>3.816269</th>\n",
       "    <th>0.288839</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.049144</th>\n",
       "    <th>3.817137</th>\n",
       "    <th>0.288672</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.047353</th>\n",
       "    <th>3.817998</th>\n",
       "    <th>0.289621</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.045847</th>\n",
       "    <th>3.817185</th>\n",
       "    <th>0.289509</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.044255</th>\n",
       "    <th>3.817633</th>\n",
       "    <th>0.290346</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.042569</th>\n",
       "    <th>3.817270</th>\n",
       "    <th>0.289676</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.041622</th>\n",
       "    <th>3.818153</th>\n",
       "    <th>0.289453</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:04 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>3.038305</th>\n",
       "    <th>3.818230</th>\n",
       "    <th>0.289565</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>3.039619</th>\n",
       "    <th>3.818743</th>\n",
       "    <th>0.289118</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>3.038204</th>\n",
       "    <th>3.819620</th>\n",
       "    <th>0.289286</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>3.037751</th>\n",
       "    <th>3.818827</th>\n",
       "    <th>0.290625</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>3.036355</th>\n",
       "    <th>3.822935</th>\n",
       "    <th>0.290513</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>3.036094</th>\n",
       "    <th>3.824201</th>\n",
       "    <th>0.289230</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>3.034132</th>\n",
       "    <th>3.825573</th>\n",
       "    <th>0.289900</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>3.031486</th>\n",
       "    <th>3.826695</th>\n",
       "    <th>0.291295</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>8</th>\n",
       "    <th>3.028876</th>\n",
       "    <th>3.830092</th>\n",
       "    <th>0.289342</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>9</th>\n",
       "    <th>3.026451</th>\n",
       "    <th>3.830688</th>\n",
       "    <th>0.289342</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>10</th>\n",
       "    <th>3.023480</th>\n",
       "    <th>3.831692</th>\n",
       "    <th>0.288393</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>11</th>\n",
       "    <th>3.020265</th>\n",
       "    <th>3.833428</th>\n",
       "    <th>0.289342</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>12</th>\n",
       "    <th>3.017920</th>\n",
       "    <th>3.833005</th>\n",
       "    <th>0.289732</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>13</th>\n",
       "    <th>3.014709</th>\n",
       "    <th>3.835399</th>\n",
       "    <th>0.288951</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>14</th>\n",
       "    <th>3.012921</th>\n",
       "    <th>3.835636</th>\n",
       "    <th>0.289062</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>15</th>\n",
       "    <th>3.009837</th>\n",
       "    <th>3.836457</th>\n",
       "    <th>0.288839</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>16</th>\n",
       "    <th>3.008352</th>\n",
       "    <th>3.837088</th>\n",
       "    <th>0.289062</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>17</th>\n",
       "    <th>3.006598</th>\n",
       "    <th>3.836449</th>\n",
       "    <th>0.289230</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>18</th>\n",
       "    <th>3.006465</th>\n",
       "    <th>3.838238</th>\n",
       "    <th>0.289342</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>19</th>\n",
       "    <th>3.005386</th>\n",
       "    <th>3.836332</th>\n",
       "    <th>0.288895</th>\n",
       "    <th>00:00</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, max_lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like LSTM's give us a boost from ~27-28% to >29% accuracy. Would be interested in tweaking this further and playing around with different LSTM architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn.save(\"lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is an attempt to get this model working with fastai's `predict` method. No such luck -- will need to investigate this further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class LSTMModelPred(nn.Module):\n",
    "     \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.i_h = nn.Embedding(nv, nh)\n",
    "#         self.h_o = nn.Linear(nh, nv)\n",
    "#         self.bn = BatchNorm1dFlat(nh)\n",
    "#         self.h = torch.zeros(1, 1, nh).cuda()\n",
    "#         self.c = torch.zeros(1, 1, nh).cuda()\n",
    "#         self.lstm = nn.LSTM(nh, nh, 1, batch_first=True, dropout=0.5) \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         res, (h, c) = self.lstm(self.i_h(x), (self.h, self.c))\n",
    "#         self.h, self.c = h.detach(), c.detach()\n",
    "#         return self.h_o(self.bn(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn2 = Learner(data, LSTMModelPred(), metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn2.load(\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn2.predict(\"the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn.predict(\"the\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
